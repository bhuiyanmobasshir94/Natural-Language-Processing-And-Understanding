{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Entity extraction using bert.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOYEiMU1tupSeTlCT1yCOLb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuiyanmobasshir94/Natural-Language-Processing-And-Understanding/blob/main/notebooks/Entity_extraction_using_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHhZGnEgVg4D",
        "outputId": "260bd449-c2b2-4e54-dd3a-6b099ecf6927"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4YdwoyFRIZk",
        "outputId": "3f256bbb-838d-4bba-f0e2-7f61c1f2f104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.py\n"
          ]
        }
      ],
      "source": [
        "%%file config.py\n",
        "import transformers\n",
        "\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 10\n",
        "# BASE_MODEL_PATH = \"../input/bert_base_uncased\"\n",
        "MODEL_PATH = \"model.bin\"\n",
        "TRAINING_FILE = \"input/ner_dataset.csv\"\n",
        "TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    do_lower_case=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file dataset.py\n",
        "import config\n",
        "import torch\n",
        "\n",
        "\n",
        "class EntityDataset:\n",
        "    def __init__(self, texts, pos, tags):\n",
        "        # texts: [[\"hi\", \",\", \"my\", \"name\", \"is\", \"abhishek\"], [\"hello\".....]]\n",
        "        # pos/tags: [[1 2 3 4 1 5], [....].....]]\n",
        "        self.texts = texts\n",
        "        self.pos = pos\n",
        "        self.tags = tags\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        text = self.texts[item]\n",
        "        pos = self.pos[item]\n",
        "        tags = self.tags[item]\n",
        "\n",
        "        ids = []\n",
        "        target_pos = []\n",
        "        target_tag =[]\n",
        "\n",
        "        for i, s in enumerate(text):\n",
        "            inputs = config.TOKENIZER.encode(\n",
        "                s,\n",
        "                add_special_tokens=False\n",
        "            )\n",
        "            # abhishek: ab ##hi ##sh ##ek\n",
        "            input_len = len(inputs)\n",
        "            ids.extend(inputs)\n",
        "            target_pos.extend([pos[i]] * input_len)\n",
        "            target_tag.extend([tags[i]] * input_len)\n",
        "\n",
        "        ids = ids[:config.MAX_LEN - 2]\n",
        "        target_pos = target_pos[:config.MAX_LEN - 2]\n",
        "        target_tag = target_tag[:config.MAX_LEN - 2]\n",
        "\n",
        "        ids = [101] + ids + [102]\n",
        "        target_pos = [0] + target_pos + [0]\n",
        "        target_tag = [0] + target_tag + [0]\n",
        "\n",
        "        mask = [1] * len(ids)\n",
        "        token_type_ids = [0] * len(ids)\n",
        "\n",
        "        padding_len = config.MAX_LEN - len(ids)\n",
        "\n",
        "        ids = ids + ([0] * padding_len)\n",
        "        mask = mask + ([0] * padding_len)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "        target_pos = target_pos + ([0] * padding_len)\n",
        "        target_tag = target_tag + ([0] * padding_len)\n",
        "\n",
        "        return {\n",
        "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            \"target_pos\": torch.tensor(target_pos, dtype=torch.long),\n",
        "            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n",
        "        }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEZ5TbBZRNA2",
        "outputId": "ed2e9ede-9500-41cb-a7c6-9180bb957b22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file engine.py\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        _, _, loss = model(**data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        final_loss += loss.item()\n",
        "    return final_loss / len(data_loader)\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        _, _, loss = model(**data)\n",
        "        final_loss += loss.item()\n",
        "    return final_loss / len(data_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p5390TlRa9m",
        "outputId": "e60781ba-36a1-44af-8e5c-5802a9bc8824"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file model.py\n",
        "import config\n",
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "\n",
        "def loss_fn(output, target, mask, num_labels):\n",
        "    lfn = nn.CrossEntropyLoss()\n",
        "    active_loss = mask.view(-1) == 1\n",
        "    active_logits = output.view(-1, num_labels)\n",
        "    active_labels = torch.where(\n",
        "        active_loss,\n",
        "        target.view(-1),\n",
        "        torch.tensor(lfn.ignore_index).type_as(target)\n",
        "    )\n",
        "    loss = lfn(active_logits, active_labels)\n",
        "    return loss\n",
        "\n",
        "\n",
        "class EntityModel(nn.Module):\n",
        "    def __init__(self, num_tag, num_pos):\n",
        "        super(EntityModel, self).__init__()\n",
        "        self.num_tag = num_tag\n",
        "        self.num_pos = num_pos\n",
        "        self.bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\",return_dict=False)\n",
        "        self.bert_drop_1 = nn.Dropout(0.3)\n",
        "        self.bert_drop_2 = nn.Dropout(0.3)\n",
        "        self.out_tag = nn.Linear(768, self.num_tag)\n",
        "        self.out_pos = nn.Linear(768, self.num_pos)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids, target_pos, target_tag):\n",
        "        o1, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "\n",
        "        bo_tag = self.bert_drop_1(o1)\n",
        "        bo_pos = self.bert_drop_2(o1)\n",
        "\n",
        "        tag = self.out_tag(bo_tag)\n",
        "        pos = self.out_pos(bo_pos)\n",
        "\n",
        "        loss_tag = loss_fn(tag, target_tag, mask, self.num_tag)\n",
        "        loss_pos = loss_fn(pos, target_pos, mask, self.num_pos)\n",
        "\n",
        "        loss = (loss_tag + loss_pos) / 2\n",
        "\n",
        "        return tag, pos, loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzfILdDiRkds",
        "outputId": "d8a53959-5ed5-4ce9-bede-ced05b60cf58"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file predict.py\n",
        "import numpy as np\n",
        "\n",
        "import joblib\n",
        "import torch\n",
        "\n",
        "import config\n",
        "import dataset\n",
        "import engine\n",
        "from model import EntityModel\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    meta_data = joblib.load(\"meta.bin\")\n",
        "    enc_pos = meta_data[\"enc_pos\"]\n",
        "    enc_tag = meta_data[\"enc_tag\"]\n",
        "\n",
        "    num_pos = len(list(enc_pos.classes_))\n",
        "    num_tag = len(list(enc_tag.classes_))\n",
        "\n",
        "    sentence = \"\"\"\n",
        "    abhishek is going to india\n",
        "    \"\"\"\n",
        "    tokenized_sentence = config.TOKENIZER.encode(sentence)\n",
        "\n",
        "    sentence = sentence.split()\n",
        "    print(sentence)\n",
        "    print(tokenized_sentence)\n",
        "\n",
        "    test_dataset = dataset.EntityDataset(\n",
        "        texts=[sentence], \n",
        "        pos=[[0] * len(sentence)], \n",
        "        tags=[[0] * len(sentence)]\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n",
        "    model.load_state_dict(torch.load(config.MODEL_PATH))\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        data = test_dataset[0]\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device).unsqueeze(0)\n",
        "        tag, pos, _ = model(**data)\n",
        "\n",
        "        print(\n",
        "            enc_tag.inverse_transform(\n",
        "                tag.argmax(2).cpu().numpy().reshape(-1)\n",
        "            )[:len(tokenized_sentence)]\n",
        "        )\n",
        "        print(\n",
        "            enc_pos.inverse_transform(\n",
        "                pos.argmax(2).cpu().numpy().reshape(-1)\n",
        "            )[:len(tokenized_sentence)]\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3Mnv9uCRvhO",
        "outputId": "53bfd0aa-a365-41b3-ab9f-d559d5e456d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file train.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import joblib\n",
        "import torch\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import config\n",
        "import dataset\n",
        "import engine\n",
        "from model import EntityModel\n",
        "\n",
        "\n",
        "def process_data(data_path):\n",
        "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
        "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "\n",
        "    enc_pos = preprocessing.LabelEncoder()\n",
        "    enc_tag = preprocessing.LabelEncoder()\n",
        "\n",
        "    df.loc[:, \"POS\"] = enc_pos.fit_transform(df[\"POS\"])\n",
        "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
        "\n",
        "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "    pos = df.groupby(\"Sentence #\")[\"POS\"].apply(list).values\n",
        "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "    return sentences, pos, tag, enc_pos, enc_tag\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sentences, pos, tag, enc_pos, enc_tag = process_data(config.TRAINING_FILE)\n",
        "    \n",
        "    meta_data = {\n",
        "        \"enc_pos\": enc_pos,\n",
        "        \"enc_tag\": enc_tag\n",
        "    }\n",
        "\n",
        "    joblib.dump(meta_data, \"meta.bin\")\n",
        "\n",
        "    num_pos = len(list(enc_pos.classes_))\n",
        "    num_tag = len(list(enc_tag.classes_))\n",
        "\n",
        "    (\n",
        "        train_sentences,\n",
        "        test_sentences,\n",
        "        train_pos,\n",
        "        test_pos,\n",
        "        train_tag,\n",
        "        test_tag\n",
        "    ) = model_selection.train_test_split(sentences, pos, tag, random_state=42, test_size=0.1)\n",
        "\n",
        "    train_dataset = dataset.EntityDataset(\n",
        "        texts=train_sentences, pos=train_pos, tags=train_tag\n",
        "    )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=config.TRAIN_BATCH_SIZE, num_workers=4\n",
        "    )\n",
        "\n",
        "    valid_dataset = dataset.EntityDataset(\n",
        "        texts=test_sentences, pos=test_pos, tags=test_tag\n",
        "    )\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=config.VALID_BATCH_SIZE, num_workers=1\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n",
        "    model.to(device)\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_parameters = [\n",
        "        {\n",
        "            \"params\": [\n",
        "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "            \"weight_decay\": 0.001,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [\n",
        "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    num_train_steps = int(len(train_sentences) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
        "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
        "    )\n",
        "\n",
        "    best_loss = np.inf\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        train_loss = engine.train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
        "        test_loss = engine.eval_fn(valid_data_loader, model, device)\n",
        "        print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
        "        if test_loss < best_loss:\n",
        "            torch.save(model.state_dict(), config.MODEL_PATH)\n",
        "            best_loss = test_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cytXTA_ER3st",
        "outputId": "acb2c2cc-9ca2-4b9e-9ed2-b859be020e10"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir input"
      ],
      "metadata": {
        "id": "TJKOrM3tSA79"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEQGaaIzS_Mf",
        "outputId": "91aac701-3da0-4308-e42a-0c456ec1b13c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 226k/226k [00:00<00:00, 2.21MB/s]\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 29.8kB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 2.71MB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 354kB/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 36, in <module>\n",
            "    sentences, pos, tag, enc_pos, enc_tag = process_data(config.TRAINING_FILE)\n",
            "  File \"train.py\", line 20, in process_data\n",
            "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\", line 688, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\", line 454, in _read\n",
            "    parser = TextFileReader(fp_or_buf, **kwds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\", line 948, in __init__\n",
            "    self._make_engine(self.engine)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\", line 1180, in _make_engine\n",
            "    self._engine = CParserWrapper(self.f, **self.options)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\", line 1993, in __init__\n",
            "    src = open(src, \"rb\")\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'input/ner_dataset.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OlaOtUdfVbNq"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}